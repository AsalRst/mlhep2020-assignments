{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(800, noise=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_and_model_decision(\n",
    "    X, y,\n",
    "    model=None,\n",
    "    x1limits=(-2.0, 3.0),\n",
    "    x2limits=(-1.5, 2.0),\n",
    "    npoints=250,\n",
    "    make_figure=True\n",
    "):\n",
    "    if make_figure:\n",
    "        plt.figure(figsize=(6, 6), dpi=100)\n",
    "\n",
    "    plt.scatter(*X.T, c=y, cmap='PiYG')\n",
    "    \n",
    "    plt.xlim(*x1limits)\n",
    "    plt.ylim(*x2limits)\n",
    "    \n",
    "    if model is not None:\n",
    "        xx1, xx2 = np.meshgrid(\n",
    "            np.linspace(*x1limits, npoints),\n",
    "            np.linspace(*x2limits, npoints)\n",
    "        )\n",
    "        yy = model.predict(\n",
    "            np.stack([xx1.ravel(), xx2.ravel()], axis=1)\n",
    "        ).reshape(xx1.shape)\n",
    "        \n",
    "        plt.contourf(xx1, xx2, yy, levels=30, cmap='PiYG', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_and_model_decision(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_num_plots = 4\n",
    "\n",
    "inverse_regularization_powers = np.logspace(12, -3, sqrt_num_plots**2)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "models = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for i, C in enumerate(tqdm(inverse_regularization_powers)):\n",
    "    model = make_pipeline(\n",
    "        MinMaxScaler((-0.9, 0.9)),\n",
    "        PolynomialFeatures(10, include_bias=False),\n",
    "        LogisticRegression(penalty='l2', C=C, solver='newton-cg', max_iter=100000)\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train);\n",
    "    models.append(model)\n",
    "    train_scores.append((model.predict(X_train) == y_train).mean())\n",
    "    test_scores.append((model.predict(X_test) == y_test).mean())\n",
    "    \n",
    "    plt.subplot(sqrt_num_plots, sqrt_num_plots, i + 1)\n",
    "    plot_data_and_model_decision(X_train, y_train, model, make_figure=False)\n",
    "\n",
    "\n",
    "train_scores = np.array(train_scores)\n",
    "test_scores = np.array(test_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 4.5), dpi=100)\n",
    "plt.plot(inverse_regularization_powers, 1. - train_scores, label='train')\n",
    "plt.plot(inverse_regularization_powers, 1. - test_scores, label='test')\n",
    "plt.legend();\n",
    "plt.xlabel('inv. regularization strength')\n",
    "plt.ylabel('misclassification rate')\n",
    "plt.xscale('log');\n",
    "\n",
    "\n",
    "weights = np.array([model.steps[-1][1].coef_ for model in models]).squeeze()\n",
    "plt.figure(figsize=(8, 4.5), dpi=100)\n",
    "for w in weights.T:\n",
    "    plt.plot(inverse_regularization_powers, np.abs(w));\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('inv. regularization strength')\n",
    "plt.ylabel('abs(weight)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_num_plots = 8\n",
    "\n",
    "inverse_regularization_powers = np.logspace(4, -2, sqrt_num_plots**2)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "models = []\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for i, C in enumerate(tqdm(inverse_regularization_powers)):\n",
    "    model = make_pipeline(\n",
    "        MinMaxScaler((-0.9, 0.9)),\n",
    "        PolynomialFeatures(10, include_bias=False),\n",
    "        LogisticRegression(penalty='l1', C=C, solver='saga', max_iter=10000)\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train);\n",
    "    models.append(model)\n",
    "    train_scores.append((model.predict(X_train) == y_train).mean())\n",
    "    test_scores.append((model.predict(X_test) == y_test).mean())\n",
    "    \n",
    "    plt.subplot(sqrt_num_plots, sqrt_num_plots, i + 1)\n",
    "    plot_data_and_model_decision(X_train, y_train, model, make_figure=False)\n",
    "\n",
    "\n",
    "train_scores = np.array(train_scores)\n",
    "test_scores = np.array(test_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 4.5), dpi=100)\n",
    "plt.plot(inverse_regularization_powers, 1. - train_scores, label='train')\n",
    "plt.plot(inverse_regularization_powers, 1. - test_scores, label='test')\n",
    "plt.legend();\n",
    "plt.xlabel('inv. regularization strength')\n",
    "plt.ylabel('misclassification rate')\n",
    "plt.xscale('log');\n",
    "\n",
    "\n",
    "weights = np.array([model.steps[-1][1].coef_ for model in models]).squeeze()\n",
    "plt.figure(figsize=(8, 4.5), dpi=100)\n",
    "for w in weights.T:\n",
    "    plt.plot(inverse_regularization_powers, np.log1p(np.abs(w)));\n",
    "plt.xscale('log')\n",
    "plt.xlabel('inv. regularization strength')\n",
    "plt.ylabel('log(1 + abs(weight))');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "## Final quest\n",
    "\n",
    "Build a model for the regression problem below to beat the score of test MSE < 0.0005.\n",
    "You may try using `sklearn.linear_model.Ridge` or `sklearn.linear_model.Lasso` linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18c6b08d75f33a51df54b59b31703043",
     "grade": false,
     "grade_id": "cell-cee5f40e724ecac5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import sph_harm as unknown_magic_function\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "def gen_data(N=2000):\n",
    "    X1 = np.random.uniform(-1, 1, size=(N, 2))\n",
    "    X2 = np.random.randint(0, 3, size=(N, 1))\n",
    "    X3 = np.round(np.random.uniform(-1, 1, size=(N, 1)) * X2).astype(int)\n",
    "    \n",
    "    y = unknown_magic_function(X3[:,0], X2[:,0], *np.arccos(X1.T)).imag\n",
    "    \n",
    "    X = np.concatenate([X1, X2, X3], axis=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = gen_data()\n",
    "X_test, y_test = gen_data()\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(((model.predict(X_train) - y_train)**2).mean())\n",
    "print(((model.predict(X_test) - y_test)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74e7266835663dfc7ce8f8580041ffa8",
     "grade": true,
     "grade_id": "FitScore",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import sph_harm as unknown_magic_function\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "def gen_data(N=2000):\n",
    "    X1 = np.random.uniform(-1, 1, size=(N, 2))\n",
    "    X2 = np.random.randint(0, 3, size=(N, 1))\n",
    "    X3 = np.round(np.random.uniform(-1, 1, size=(N, 1)) * X2).astype(int)\n",
    "    \n",
    "    y = unknown_magic_function(X3[:,0], X2[:,0], *np.arccos(X1.T)).imag\n",
    "    \n",
    "    X = np.concatenate([X1, X2, X3], axis=1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = gen_data()\n",
    "X_test, y_test = gen_data()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_mse = ((model.predict(X_test) - y_test)**2).mean()\n",
    "\n",
    "assert test_mse < 0.0005, test_mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
