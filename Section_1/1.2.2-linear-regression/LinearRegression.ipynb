{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore the numeric solution for the Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our own class for the linear regression model. To make it more flexible, we'll make the code independent of whatever loss function we'll decide to optimize using gradient descent. So let's start by defining the loss functions, together with their gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote the true outcomes $y=\\left\\{y_1, \\ldots, y_N\\right\\}$ and the predictions $\\hat{y}=\\left\\{\\hat{y}_1, \\ldots, \\hat{y}_N\\right\\}$, then MSE loss may be defined as:\n",
    "$$\\text{MSE}(y, \\hat{y})=\\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y}_i)^2$$\n",
    "and its gradient written in vector notation:\n",
    "$$\\frac{\\partial\\text{MSE}(y, \\hat{y})}{\\partial\\hat{y}}=\\frac{2}{N}(\\hat{y}-y)$$\n",
    "(you can check that it holds componentwise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll have a class for the MSE loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def evaluate(self, y, yhat):\n",
    "        return np.mean((yhat - y)**2)\n",
    "    \n",
    "    def grad(self, y, yhat):\n",
    "        return 2 * (yhat - y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also code the MAE loss in a similar manner. MAE loss is defined as:\n",
    "$$\\text{MAE}(y, \\hat{y})=\\frac{1}{N}\\sum_{i=1}^N|y_i-\\hat{y}_i|$$\n",
    "and its gradient is\n",
    "$$\\frac{\\partial\\text{MAE}(y, \\hat{y})}{\\partial\\hat{y}}=~?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...haha, no answer this time. I'm sure you can do it yourself. Think what it is and try to code it below.\n",
    "\n",
    "*Hint: you may find some of these numpy functions useful: `np.sign` or `np.where`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dbcfe0492b9e3be348def0bd2f42f64",
     "grade": false,
     "grade_id": "cell-ffc94adc34b60763",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MAELoss:\n",
    "    def evaluate(self, y, yhat):\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def grad(self, y, yhat):\n",
    "        # your code here\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When ready, check your solution with the assertions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96af98abae09ea2f6c61435bc99b6120",
     "grade": true,
     "grade_id": "MAELoss",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mae_loss = MAELoss()\n",
    "\n",
    "dummy_y = np.array([[ 2.96568876, -0.44775447, -2.08341949],\n",
    "                    [-0.19653415, -1.35417292,  2.38227265],\n",
    "                    [-0.3718978 ,  0.94750511, -0.34423604],\n",
    "                    [-0.5259521 , -0.61043298,  0.08949304],\n",
    "                    [-0.1841808 , -0.86844919,  0.05762396]])\n",
    "dummy_yhat = np.array([[ 1.97139419,  0.64517944,  0.42927158],\n",
    "                       [-0.13414484,  0.51593211, -1.12827816],\n",
    "                       [ 1.2921285 , -0.46942819, -0.32441611],\n",
    "                       [-0.66332328,  1.66408605, -0.63113832],\n",
    "                       [ 1.04913489, -2.4929845 ,  0.37787085]])\n",
    "\n",
    "np.testing.assert_equal(\n",
    "    mae_loss.evaluate(dummy_y, dummy_yhat).shape,\n",
    "    ()\n",
    ")\n",
    "np.testing.assert_almost_equal(\n",
    "    mae_loss.evaluate(dummy_y, dummy_yhat),\n",
    "    1.2969575793333334\n",
    ")\n",
    "\n",
    "np.testing.assert_equal(\n",
    "    mae_loss.grad(dummy_y, dummy_yhat).shape,\n",
    "    (5, 3)\n",
    ")\n",
    "np.testing.assert_almost_equal(\n",
    "    mae_loss.grad(dummy_y, dummy_yhat),\n",
    "    np.array([[-0.2,  0.2,  0.2],\n",
    "              [ 0.2,  0.2, -0.2],\n",
    "              [ 0.2, -0.2,  0.2],\n",
    "              [-0.2,  0.2, -0.2],\n",
    "              [ 0.2, -0.2,  0.2]])\n",
    ")\n",
    "\n",
    "dummy_y = np.array([-0.5259521 , -0.61043298,  0.08949304, -0.1841808 , -0.86844919,  0.05762396])\n",
    "dummy_yhat = np.array([1.97139419,  0.64517944,  0.42927158, -0.13414484,  0.51593211, -1.12827816])\n",
    "np.testing.assert_equal(\n",
    "    mae_loss.grad(dummy_y, dummy_yhat).shape,\n",
    "    (6,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define a function for calculating linear model prediction. `features` is the design matrix of shape `(N, d)`, where `d` is the number of features and `N` â€“ the number of objects in the dataset. `parameters` is the vector or matrix of parameters either of shape `(d,)` (in case the targets are 1-dimensional), or of shape `(d, m)` (in case the targets are `m`-dimensional).\n",
    "\n",
    "*Hint: use `np.matmul` or `@` for matrix multiplication.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0da59c80bbb1472f161c5e268ce369f9",
     "grade": false,
     "grade_id": "cell-a73089e510bf5b0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_model_prediction(features, parameters):\n",
    "    # your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your solution passes the assertions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63dd3d64e1c32e72e4b6b538145cd83f",
     "grade": true,
     "grade_id": "LinearPrediction",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_features = np.array([[0.96276835, 0.07560504, 0.36568075],\n",
    "                           [0.22538127, 0.19583308, 0.14080295],\n",
    "                           [0.62241364, 0.78132186, 0.5782984 ],\n",
    "                           [0.14696271, 0.81112377, 0.63595471],\n",
    "                           [0.38816571, 0.6741303 , 0.25990498],\n",
    "                           [0.95793412, 0.52965972, 0.24189329]])\n",
    "dummy_parameters = np.array([[0.50060357, 0.67962033],\n",
    "                             [0.07623922, 0.27470382],\n",
    "                             [0.80696549, 0.45966487]])\n",
    "np.testing.assert_almost_equal(\n",
    "    linear_model_prediction(dummy_features, dummy_parameters),\n",
    "    np.array([[0.78282109, 0.84317653],\n",
    "              [0.24137995, 0.27169196],\n",
    "              [0.83781671, 0.90346052],\n",
    "              [0.64860301, 0.61502368],\n",
    "              [0.45544666, 0.56846067],\n",
    "              [0.71512562, 0.9077209 ]])\n",
    ")\n",
    "np.testing.assert_equal(\n",
    "    linear_model_prediction(dummy_features, dummy_parameters).shape,\n",
    "    (6, 2)\n",
    ")\n",
    "dummy_features = np.array([[0.96276835, 0.07560504, 0.36568075],\n",
    "                           [0.22538127, 0.19583308, 0.14080295],\n",
    "                           [0.62241364, 0.78132186, 0.5782984 ],\n",
    "                           [0.14696271, 0.81112377, 0.63595471],\n",
    "                           [0.38816571, 0.6741303 , 0.25990498],\n",
    "                           [0.95793412, 0.52965972, 0.24189329]])\n",
    "dummy_parameters = np.array([0.50060357, 0.07623922, 0.80696549])\n",
    "np.testing.assert_almost_equal(\n",
    "    linear_model_prediction(dummy_features, dummy_parameters),\n",
    "    np.array([0.78282109, 0.24137995, 0.83781671, 0.64860301, 0.45544666,\n",
    "       0.71512562])\n",
    ")\n",
    "np.testing.assert_equal(\n",
    "    linear_model_prediction(dummy_features, dummy_parameters).shape,\n",
    "    (6,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll optimize our model with gradient descent optimization. We already defined the gradients of the losses wrt the model outputs (`loss_gradients` in the cell below):\n",
    "$$\\frac{\\partial\\text{Loss}(y, \\hat{y})}{\\partial\\hat{y}}$$\n",
    "Now we need to calculate the gradient of the loss wrt the model parameters:\n",
    "$$\\hat{y} = XW$$\n",
    "$$\\frac{\\partial\\text{Loss}(y, \\hat{y})}{\\partial W}=~?$$\n",
    "\n",
    "*Hint: try to analyze the shapes of all the variables and the shape of the result you want to get.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09415782fb8701875057db766e36333d",
     "grade": false,
     "grade_id": "cell-8a304322f15a0afd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_model_gradients(features, loss_gradients):\n",
    "    # your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the assertions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "791289da378b8b10c082d0c2d2644da0",
     "grade": true,
     "grade_id": "LinearGradients",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_features = np.array([[0.96276835, 0.07560504, 0.36568075],\n",
    "                           [0.22538127, 0.19583308, 0.14080295],\n",
    "                           [0.62241364, 0.78132186, 0.5782984 ],\n",
    "                           [0.14696271, 0.81112377, 0.63595471],\n",
    "                           [0.38816571, 0.6741303 , 0.25990498]])\n",
    "\n",
    "dummy_gradients = np.array([[ 2.96568876, -2.08341949],\n",
    "                            [-0.19653415,  2.38227265],\n",
    "                            [-0.3718978 , -0.34423604],\n",
    "                            [-0.5259521 ,  0.08949304],\n",
    "                            [-0.1841808 ,  0.05762396]])\n",
    "\n",
    "np.testing.assert_equal(\n",
    "    linear_model_gradients(dummy_features, dummy_gradients).shape,\n",
    "    (3, 2)\n",
    ")\n",
    "np.testing.assert_almost_equal(\n",
    "    linear_model_gradients(dummy_features, dummy_gradients),\n",
    "    np.array([[ 2.43071388, -1.64766813],\n",
    "              [-0.65561286,  0.15148762],\n",
    "              [ 0.45940358, -0.55361626]])\n",
    ")\n",
    "\n",
    "dummy_gradients = np.array([-2.08341949, 2.38227265, -0.34423604, 0.08949304, 0.05762396])\n",
    "np.testing.assert_equal(\n",
    "    linear_model_gradients(dummy_features, dummy_gradients).shape,\n",
    "    (3,)\n",
    ")\n",
    "np.testing.assert_almost_equal(\n",
    "    linear_model_gradients(dummy_features, dummy_gradients),\n",
    "    np.array([-1.64766813,  0.15148762, -0.55361626])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this tutorial, this function will only be used for 1-dimensional problems. Having a single feature â€“ design matrix `X` of shape `(N, 1)` â€“ you want to raise it to all different powers up to `power` (inclusive), starting from power 0 (to account for the bias term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7def5ccce72ca1b83f5e594c4f4b8875",
     "grade": false,
     "grade_id": "cell-128f3ec4a62f082b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_expand(X, power, limits=None):\n",
    "    \n",
    "    # You don't want to raise large numbers to large powers,\n",
    "    # so we might need to normalize the feature\n",
    "    if limits is not None:\n",
    "        X = (X - limits[0]) / (limits[1] - limits[0])\n",
    "        \n",
    "    # your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, automatic checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5634833c0b8cf2bc3ae09dbdf8ebdff2",
     "grade": true,
     "grade_id": "PolynomialExpand",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_features = np.array([-0.29242939, -1.51807567, -1.42512742, -2.39793806,  0.69249136]).reshape(-1, 1)\n",
    "np.testing.assert_almost_equal(\n",
    "    polynomial_expand(dummy_features, 5, (-2, 2)),\n",
    "    np.array([[ 1.00000000e+00,  4.26892652e-01,  1.82237337e-01,\n",
    "                7.77957801e-02,  3.32104469e-02,  1.41772958e-02],\n",
    "              [ 1.00000000e+00,  1.20481083e-01,  1.45156912e-02,\n",
    "                1.74886619e-03,  2.10705292e-04,  2.53860017e-05],\n",
    "              [ 1.00000000e+00,  1.43718145e-01,  2.06549052e-02,\n",
    "                2.96848466e-03,  4.26625109e-04,  6.13137693e-05],\n",
    "              [ 1.00000000e+00, -9.94845150e-02,  9.89716872e-03,\n",
    "               -9.84615030e-04,  9.79539488e-05, -9.74490109e-06],\n",
    "              [ 1.00000000e+00,  6.73122840e-01,  4.53094358e-01,\n",
    "                3.04988161e-01,  2.05294497e-01,  1.38188415e-01]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some classes and functions for our convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a class for our linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    def __init__(self,\n",
    "                 n_features,  # - we need to know how many weights to initialize\n",
    "                 n_targets=1, # - let's also make it possible to have multidimentional target\n",
    "                              #   (it just means having a matrix of weights instead of just\n",
    "                              #   a vector)\n",
    "                 loss=MSELoss(),\n",
    "                 preprocessing_steps=None # - this will be a list of functions to apply to the \n",
    "                                          #   features (e.g. to make polynomial expansion)\n",
    "                ):\n",
    "        \n",
    "        # initializing the weights:\n",
    "        self.W = np.zeros(\n",
    "            shape=(n_features, n_targets),\n",
    "            dtype=float\n",
    "        ).squeeze() # `squeeze()` removes unit dimensions. E.g. for 1-dimensional target\n",
    "                    # it will make W.shape = (d,) instead of (d, 1).\n",
    "\n",
    "        self.loss = loss\n",
    "\n",
    "        if preprocessing_steps is None:\n",
    "            self.preprocessing_steps = []\n",
    "        else:\n",
    "            self.preprocessing_steps = preprocessing_steps\n",
    "\n",
    "        # We'll use gradient descent with momentum to spice things up\n",
    "        # (see https://distill.pub/2017/momentum/ for more info)\n",
    "        self.momentum = np.zeros_like(self.W)\n",
    "\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        # apply all the preprocessing functions\n",
    "        for step in self.preprocessing_steps:\n",
    "            X = step(X)\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        # using the function you defined above:\n",
    "        return linear_model_prediction(\n",
    "            self.preprocess(X), self.W\n",
    "        )\n",
    "\n",
    "    def evaluate_loss(self, X, y):\n",
    "        return self.loss.evaluate(\n",
    "            y,\n",
    "            self.predict(X)\n",
    "        )\n",
    "\n",
    "    def learning_step(self, X, y, learning_rate=0.01, beta=0.9):\n",
    "        gradients = linear_model_gradients(\n",
    "            self.preprocess(X),\n",
    "            self.loss.grad(\n",
    "                y,\n",
    "                self.predict(X)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Instead of stepping towards the negative gradient, we'll update\n",
    "        # the current value of momentum and step towards the negative momentum.\n",
    "        self.momentum = beta * self.momentum + (1 - beta) * gradients\n",
    "        self.W -= learning_rate * self.momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility class to keep the data and do the plotting\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self,\n",
    "                 X_train,\n",
    "                 y_train,\n",
    "                 X_validation,\n",
    "                 y_validation,\n",
    "                 true_function):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_validation = X_validation\n",
    "        self.y_validation = y_validation\n",
    "        self.true_function = true_function\n",
    "    \n",
    "    def plot(self, model=None):\n",
    "        limits = (\n",
    "            min(self.X_train.min(), self.X_validation.min()),\n",
    "            max(self.X_train.max(), self.X_validation.max())\n",
    "        )\n",
    "        delta = 0.1 * (limits[1] - limits[0])\n",
    "        \n",
    "        xx = np.linspace(\n",
    "            limits[0] - delta,\n",
    "            limits[1] + delta,\n",
    "            100\n",
    "        )\n",
    "\n",
    "        plt.scatter(self.X_train, self.y_train, label='training data')\n",
    "        plt.plot(xx, self.true_function(xx), label='truth')\n",
    "        if model is not None:\n",
    "            plt.plot(xx, model.predict(xx.reshape(-1, 1)), label='prediction')\n",
    "            plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a number of gradient descent steps. Optionally plot the prediction and learning curves\n",
    "\n",
    "def train_model(model,\n",
    "                dataset,\n",
    "                num_steps=500,\n",
    "                learning_rate=0.01,\n",
    "                do_plot=True):\n",
    "    loss_train = []\n",
    "    loss_validation = []\n",
    "    for _ in trange(num_steps):\n",
    "        model.learning_step(dataset.X_train, dataset.y_train, learning_rate)\n",
    "        loss_train.append(\n",
    "            model.evaluate_loss(dataset.X_train, dataset.y_train)\n",
    "        )\n",
    "        loss_validation.append(\n",
    "            model.evaluate_loss(dataset.X_validation, dataset.y_validation)\n",
    "        )\n",
    "    \n",
    "    if do_plot:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(121)\n",
    "        dataset.plot(model)\n",
    "\n",
    "        plt.subplot(122)\n",
    "\n",
    "        plt.plot(loss_train, label='train loss')\n",
    "        plt.plot(loss_validation, label='validation loss')\n",
    "        plt.legend();\n",
    "    \n",
    "    return loss_train, loss_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to simulate simple 1-D datasets\n",
    "\n",
    "def generate_data(N_train, N_validation, true_function, limits, noize_func, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.uniform(limits[0], limits[1], size=N_train+N_validation)\n",
    "    y = true_function(X)\n",
    "    y += noize_func(X)\n",
    "    X = X.reshape(-1, 1)\n",
    "    return Dataset(X[:N_train],\n",
    "                   y[:N_train],\n",
    "                   X[N_train:],\n",
    "                   y[N_train:],\n",
    "                   true_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #1: fitting a parabola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = generate_data(N_train=30,\n",
    "                          N_validation=30,\n",
    "                          true_function=lambda x: (x + 1.5)**2, # <----- here's where we define the function\n",
    "                                                                #        you may want to play around with it\n",
    "                          limits=(-5, 5),\n",
    "                          noize_func=lambda x: np.random.normal(size=len(x)) * 3.)\n",
    "dataset_1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(\n",
    "    n_features=3,\n",
    "    preprocessing_steps=[\n",
    "        lambda x: polynomial_expand(x, power=2, limits=(-6, 6))\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_model(model,\n",
    "            dataset_1,\n",
    "            num_steps=2000,\n",
    "            learning_rate=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the learning rate, number of steps, the underlying true function or the noize power to see how it affects the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #2: Linear function + outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = generate_data(N_train=300,\n",
    "                          N_validation=300,\n",
    "                          true_function=lambda x: 3.25 * x - 1.47,\n",
    "                          limits=(-4., 20.),\n",
    "                          noize_func=lambda x: (\n",
    "                              np.random.normal(size=len(x)) * 4. +\n",
    "                              np.where(\n",
    "                                  x < 15,\n",
    "                                  0.,\n",
    "                                  -60 + np.random.normal(size=len(x)) * 10\n",
    "                              ) * (np.random.uniform(size=len(x)) < 0.4).astype(int)\n",
    "                          ))\n",
    "\n",
    "dataset_2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(\n",
    "    n_features=2,\n",
    "    preprocessing_steps=[\n",
    "        lambda x: polynomial_expand(x, power=1, limits=(-5, 20))\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_model(model,\n",
    "            dataset_2,\n",
    "            num_steps=1000,\n",
    "            learning_rate=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like outliers affect the solution a lot. What if we change our loss from MSE to MAE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(\n",
    "    n_features=2,\n",
    "    preprocessing_steps=[\n",
    "        lambda x: polynomial_expand(x, power=1, limits=(-5, 20))\n",
    "    ],\n",
    "    loss=MAELoss()\n",
    ")\n",
    "\n",
    "train_model(model,\n",
    "            dataset_2,\n",
    "            num_steps=2000,\n",
    "            learning_rate=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #3: reconstructing faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll try to use our linear model to reconstruct right halves of human faces from the left halves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces, _ = fetch_olivetti_faces(return_X_y=True)\n",
    "faces = faces.reshape(-1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tear images in halves\n",
    "def split(images):\n",
    "    return images[:,:,:32], images[:,:,32:]\n",
    "\n",
    "# glue them back together\n",
    "def glue(X, Y):\n",
    "    return np.concatenate([X, Y], axis=2)\n",
    "\n",
    "# unwrap height and width into a single axis\n",
    "def vectorize(X):\n",
    "    return X.reshape(-1, 64 * 32)\n",
    "\n",
    "# wrap single axis back into height and width\n",
    "def imagize(X):\n",
    "    return X.reshape(-1, 64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = split(faces)\n",
    "plt.subplot(121)\n",
    "plt.imshow(X[3], cmap='gray')\n",
    "plt.subplot(122)\n",
    "plt.imshow(Y[3], cmap='gray')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(glue(X, Y)[3], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = Dataset(vectorize(X)[:350],\n",
    "                    vectorize(Y)[:350],\n",
    "                    vectorize(X)[350:],\n",
    "                    vectorize(Y)[350:],\n",
    "                    true_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(\n",
    "    32 * 64,\n",
    "    32 * 64,\n",
    ")\n",
    "\n",
    "loss_train, loss_validation = train_model(model,\n",
    "                                          dataset_3,\n",
    "                                          num_steps=300,\n",
    "                                          learning_rate=.027,\n",
    "                                          do_plot=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train, label='train')\n",
    "plt.plot(loss_validation, label='validation');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief utility function to glue the prediction back with the left half of the image for a number of pictures in the dataset, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_predictions(model):\n",
    "    pred_train = model.predict(dataset_3.X_train)\n",
    "    pred_train = glue(\n",
    "        imagize(dataset_3.X_train),\n",
    "        imagize(pred_train)\n",
    "    )\n",
    "\n",
    "    pred_validation = model.predict(dataset_3.X_validation)\n",
    "    pred_validation = glue(\n",
    "        imagize(dataset_3.X_validation),\n",
    "        imagize(pred_validation)\n",
    "    )\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(\n",
    "        pred_train[:25].reshape(5, 5, 64, 64).transpose(0, 2, 1, 3).reshape(5 * 64, 5 * 64),\n",
    "        cmap='gray'\n",
    "    )\n",
    "    plt.title(\"Training images\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(\n",
    "        pred_validation[:25].reshape(5, 5, 64, 64).transpose(0, 2, 1, 3).reshape(5 * 64, 5 * 64),\n",
    "        cmap='gray'\n",
    "    )\n",
    "    plt.title(\"Validation images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_predictions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, $X^TX$ matrix is quite ill-defined for such an example, so gradient descent optimization needs a lot of steps to converge. What we did is in fact an early stopping, so we didn't quite get into the overfitting region yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If however we use the analytical solution for the linear regression, see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "model.W = scipy.linalg.lstsq(dataset_3.X_train, dataset_3.y_train)[0]\n",
    "\n",
    "show_model_predictions(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite creepy, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sklearn way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we wrote all the learning algorithms from scratch, only relying on the vectorized numpy operations. This was very useful as an educational example.\n",
    "\n",
    "All these models are however already implemented in sklearn. So here's an example of building a sklearn pipeline with scaling and expanding the features, and then predicting with the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "\n",
    "model = make_pipeline(\n",
    "    MinMaxScaler((-1, 1)),\n",
    "    PolynomialFeatures(degree=5, include_bias=False),\n",
    "    LinearRegression(fit_intercept=True)\n",
    ")\n",
    "\n",
    "model.fit(dataset_1.X_train, dataset_1.y_train)\n",
    "\n",
    "xx = np.linspace(-5, 5, 100)\n",
    "plt.scatter(dataset_1.X_train, dataset_1.y_train, label='training data')\n",
    "plt.plot(xx, dataset_1.true_function(xx), label='truth')\n",
    "plt.plot(xx, model.predict(xx.reshape(-1, 1)), label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing things, e.g.:\n",
    " - degree of the polynomial features\n",
    " - different scalers (e.g. `sklearn.preprocessing.StandardScaler` or `sklearn.preprocessing.RobustScaler`)\n",
    " - check out `sklearn.linear_model.SGDRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
